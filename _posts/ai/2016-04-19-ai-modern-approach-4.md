---
layout: note
title: Artificial Intelligence A Modern Approach Chap 4
date: 2016-04-19 12:29:12 -0700
categories: ai
---

# Chapter 4 : Informed Search and Exploration

__best-first search__: a node is selected for expansion based on an evaluation function
__evaluation function__: usually the distnace to the goal node
__heuristic function__ (evaluation function.. almost) : a function that estimates the cost of the cheapest
  path from node n to a goal node

greedy best-first search : expand the node that is closest to the goal

#### A* Search

- A* combines functions to get a good metric

f(n) = g(n) + h(n)

Where:

  - g(n) : cost to reach node
  - h(n) : the cost from the node to the goal
  - f(n) : estimated cost of the cheapest solution through n

- A* using Tree-Search is optimal if h(n) is admissible
- Suboptimal solutions can be returned if GraphSearch is used. Fix by:
  - Discard more expensive paths to the same node
  - Ensure the optimal path is the first one visited
- A* using GraphSearch is optimal if h(n) is consistent
- If h(n) is consistent, then the values of f(n) along any path are non-decreasing
- A* is optimally efficient for any given heuristic function
- A* will often run out of memory before running out of time

- Recursive best-first search (RBFS) : mimics the operation of standard best-first
    search, but using only linear space
- RBFS and IDA* will only use a small fixed amount of memory

Algorithms that makes use of all available memory:
  - MA* (memory-bounded A*)
  - SMA* (simplified memory-bounded A*)

- memory limitations can make a problem intractable form the point of view of computation time

- different heuristics can be combined to get a composite heuristic
- __pattern database__ : store the exact solution for every possible subproblem instance

- __local search__: operate only using a single state
  - uses little memory
  - suited for some large of infinite state spaces where systematic algorithms are unsuitable
  - useful for solving pure optimization problems

- Hill-climbing : will keep climbing up until all of it's neighbors are below it
  - also called greedy local search
  - can get stuck in the following situations
    - local maxima
    - ridges
    - plateaux
- stochastic Hill-climbing
- first-choice hill-climbing
- random-restart hill climbing ("if at first you don't succeed, try, try again")
  - not suited for some real problems that have graphs that look like porcupines

- simulated annealing: randomly picks locations and if the situation is improved the
  new state is taken otherwise there is a decreasing probability for it to change even
  if the change was not positive
  - efficient and used in many cases
- __local beam search__: a local search that keeps track of the last _k_ states
  - Areas with improvement quickly have a lot of work on them

- stochastic beam search : like a local beam search except that the next successors are
  chosen at random with the probability of choosing a given successor an increasing function of it's value
- genetic algorithm : variant of stochastic bveam search were a state is generated by combining two
  parent states as opposed to one
- genetic algorithms work well when independent parts can be combined together to create more fitness

- offline search : finds a solution and then executes the solution without regard to is percepts
- online search : agent operates by interleaving computation and action

- learning real-time A*: LRTA*
- If an environment is not safely-explorable (like real life) then it is possible
  to reach a dead end. None of the algorithms in this chapter account for this and assume
  that the environment is safely explorable.
